# Research
Goal: Tie mathematics (specify the types), programming, and artificial intelligence into a relevant industry, where the implications of our research can have a massive effect if executed.


Research Ideas (link to a dataset or how to get dataset):
<br>
DOD -
<br>
DOE - 
<br>
Financial Markets - identifying anomalies in pricings of stocks (historical price data can be gathered with an API, stored in a database, where back-end spun up on AWS can perform modeling)

https://www.scs.gatech.edu/content/foundations-artificial-intelligence
Search above when closer to research Spring 2025.


# Research Papers To Read In Order Beginner (1) to Expert (20):
    Perceptrons by Frank Rosenblatt (1958)
        A foundational paper introducing the perceptron, a fundamental building block of neural networks.

    A Few Useful Things to Know About Machine Learning by Pedro Domingos (2012)
        An introductory paper that provides practical insights into machine learning.

    Playing Atari with Deep Reinforcement Learning by Volodymyr Mnih et al. (2013)
        A seminal paper demonstrating deep Q-learning's success in playing Atari games.

    ImageNet Classification with Deep Convolutional Neural Networks by Alex Krizhevsky et al. (2012)
        The pioneering paper on the use of deep convolutional neural networks for image classification, leading to the development of the AlexNet model.

    Sequence to Sequence Learning with Neural Networks by Ilya Sutskever et al. (2014)
        A fundamental paper introducing sequence-to-sequence models, essential for natural language processing and other sequential tasks.

    Generative Adversarial Networks by Ian Goodfellow et al. (2014)
        The seminal paper that introduced Generative Adversarial Networks (GANs), a powerful approach for generating realistic data samples.

    Playing Go with Deep Neural Networks and Tree Search by David Silver et al. (2016)
        A significant paper presenting the AlphaGo system, which achieved groundbreaking success in playing the game of Go.

    Deep Residual Learning for Image Recognition by Kaiming He et al. (2015)
        A landmark paper introducing residual networks (ResNets), a critical architecture for training very deep neural networks.

    Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks by Alec Radford et al. (2016)
        A key paper that introduces DCGANs, demonstrating how to generate high-quality images using GANs.

    Neural Machine Translation by Jointly Learning to Align and Translate by Dzmitry Bahdanau et al. (2014)
        A pivotal paper introducing the attention mechanism in neural machine translation, improving translation quality.

    DeepFace: Closing the Gap to Human-Level Performance in Face Verification by Yaniv Taigman et al. (2014)
        A significant work on face verification using deep neural networks, laying the foundation for many face recognition systems.

    Human-level control through deep reinforcement learning by Volodymyr Mnih et al. (2015)
        Another key paper showcasing deep reinforcement learning's success in playing Atari games and outperforming human experts.

    Mask R-CNN by Kaiming He et al. (2017)
        An influential paper introducing the Mask R-CNN model for instance segmentation tasks, significantly improving object detection methods.

    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin et al. (2018)
        A groundbreaking paper introducing the BERT model, which revolutionized natural language understanding tasks.

    Language Models are Unsupervised Multitask Learners by Alec Radford et al. (2019)
        A pivotal paper introducing GPT (Generative Pre-trained Transformer) models, setting new benchmarks in natural language processing.

    AlphaFold: Using AI for Scientific Discovery by John Jumper et al. (2020)
        A revolutionary paper introducing AlphaFold, an AI system for predicting protein folding, significantly advancing bioinformatics.

    Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov et al. (2020)
        An important paper presenting BiT, a transfer learning model achieving state-of-the-art performance on various computer vision tasks.

    Vision Transformers by Alexey Dosovitskiy et al. (2020)
        An influential paper introducing Vision Transformers (ViT), showing their effectiveness in image recognition tasks.

    Clip: Connecting Text and Images using Contrastive Learning by Alec Radford et al. (2021)
        A significant paper presenting CLIP, a multimodal model that learns to connect text and images.

    DALL-E: Creating Images from Text by Aditya Ramesh et al. (2021)
        A remarkable paper introducing DALL-E, a model that generates images based on textual descriptions.
